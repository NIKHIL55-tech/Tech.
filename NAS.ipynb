{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ce2a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "NAS for INS calibration (PyTorch-only, CSV or .pth inputs)\n",
    "\n",
    "Features:\n",
    "- Train on one dataset and test on another (CSV or .pth supported).\n",
    "- Multi-output regression (predict bias, scale factors, etc.).\n",
    "- Simple evolutionary / random-search over MLP architectures.\n",
    "- Prints training / validation / final test metrics (MSE, MAE, R2).\n",
    "- Saves best model and scalers to a .pth checkpoint.\n",
    "\n",
    "Usage examples:\n",
    "# CSV inputs:\n",
    "python nas_ins_calibration_full.py \\\n",
    "  --train_csv ./data/train_ins.csv --test_csv ./data/test_ins.csv \\\n",
    "  --target_cols bias_x,bias_y,scale_x,scale_y \\\n",
    "  --input_cols ax,ay,az,gx,gy,gz,temperature \\\n",
    "  --pop_size 6 --generations 4 --search_epochs 6 --final_epochs 40 \\\n",
    "  --device cuda\n",
    "\n",
    "# .pth inputs (saved as {'X': np_or_tensor, 'Y': ...} or tuple (X,Y)):\n",
    "python nas_ins_calibration_full.py \\\n",
    "  --train_pth ./data/train_ins.pth --test_pth ./data/test_ins.pth \\\n",
    "  --target_cols bias_x,bias_y --device cuda\n",
    "\"\"\"\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --------------------- Utilities ---------------------\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class Scaler:\n",
    "    def __init__(self, eps: float = 1e-12):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        self.eps = eps\n",
    "\n",
    "    def fit(self, arr: np.ndarray):\n",
    "        arr = np.asarray(arr, dtype=np.float64)\n",
    "        self.mean = arr.mean(axis=0, keepdims=True)\n",
    "        self.std = arr.std(axis=0, keepdims=True)\n",
    "        self.std[self.std < self.eps] = 1.0\n",
    "\n",
    "    def transform(self, arr: np.ndarray) -> np.ndarray:\n",
    "        return (np.asarray(arr, dtype=np.float64) - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, scaled: np.ndarray) -> np.ndarray:\n",
    "        return np.asarray(scaled, dtype=np.float64) * self.std + self.mean\n",
    "\n",
    "# --------------------- Metrics ---------------------\n",
    "def mse_per_output(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "    return np.mean((y_true - y_pred) ** 2, axis=0)\n",
    "\n",
    "def mae_per_output(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "    return np.mean(np.abs(y_true - y_pred), axis=0)\n",
    "\n",
    "def r2_per_output(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2, axis=0)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true, axis=0, keepdims=True)) ** 2, axis=0)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        r2 = 1.0 - ss_res / np.where(ss_tot == 0, 1.0, ss_tot)\n",
    "    r2 = np.nan_to_num(r2, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return r2\n",
    "\n",
    "def print_metrics(prefix: str, y_true: np.ndarray, y_pred: np.ndarray, target_names: List[str]):\n",
    "    mse = mse_per_output(y_true, y_pred)\n",
    "    mae = mae_per_output(y_true, y_pred)\n",
    "    r2 = r2_per_output(y_true, y_pred)\n",
    "    print(f\"\\n{prefix} metrics (per-target):\")\n",
    "    for i, name in enumerate(target_names):\n",
    "        print(f\"  {name:20s}  MSE={mse[i]:.6e}  MAE={mae[i]:.6e}  R2={r2[i]:.4f}\")\n",
    "    print(f\"  -> Average: MSE={mse.mean():.6e}  MAE={mae.mean():.6e}  R2={r2.mean():.4f}\\n\")\n",
    "\n",
    "# --------------------- Data loading helpers ---------------------\n",
    "def load_csv_as_arrays(csv_path: str, input_cols: List[str], target_cols: List[str], sep: str = ','):\n",
    "    df = pd.read_csv(csv_path, sep=sep)\n",
    "    if input_cols is None:\n",
    "        input_cols = [c for c in df.columns if c not in target_cols]\n",
    "    X = df[input_cols].values.astype(np.float64)\n",
    "    Y = df[target_cols].values.astype(np.float64)\n",
    "    return X, Y, input_cols, target_cols\n",
    "\n",
    "def load_pth_as_arrays(pth_path: str):\n",
    "    data = torch.load(pth_path, map_location='cpu')\n",
    "    X = None\n",
    "    Y = None\n",
    "    # dict-like with keys\n",
    "    if isinstance(data, dict):\n",
    "        for k in ('X', 'x', 'inputs', 'features', 'data'):\n",
    "            if k in data:\n",
    "                X = data[k]\n",
    "                break\n",
    "        for k in ('Y', 'y', 'targets', 'labels'):\n",
    "            if k in data:\n",
    "                Y = data[k]\n",
    "                break\n",
    "        # fallback: maybe tuple saved under some key\n",
    "        if X is None and 'dataset' in data and isinstance(data['dataset'], (list, tuple)) and len(data['dataset']) >= 2:\n",
    "            X, Y = data['dataset'][0], data['dataset'][1]\n",
    "    # tuple/list directly saved\n",
    "    if X is None and isinstance(data, (list, tuple)) and len(data) >= 2:\n",
    "        X, Y = data[0], data[1]\n",
    "    if X is None or Y is None:\n",
    "        raise ValueError(f\"Could not find X and Y in pth file: {pth_path}. Expected keys: X/Y, inputs/targets, or saved tuple (X,Y).\")\n",
    "    if torch.is_tensor(X):\n",
    "        X = X.cpu().numpy()\n",
    "    if torch.is_tensor(Y):\n",
    "        Y = Y.cpu().numpy()\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    Y = np.asarray(Y, dtype=np.float64)\n",
    "    return X, Y\n",
    "\n",
    "def make_dataloader(X: np.ndarray, Y: np.ndarray, batch_size: int, shuffle: bool):\n",
    "    tX = torch.from_numpy(X).float()\n",
    "    tY = torch.from_numpy(Y).float()\n",
    "    ds = TensorDataset(tX, tY)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# --------------------- Search-space & model ---------------------\n",
    "class ArchSpec:\n",
    "    def __init__(self, num_layers: int, hidden_dims: List[int], activation: str, dropout: float, use_bn: bool):\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.activation = activation\n",
    "        self.dropout = float(dropout)\n",
    "        self.use_bn = use_bn\n",
    "\n",
    "    def mutate(self):\n",
    "        child = deepcopy(self)\n",
    "        if random.random() < 0.4:\n",
    "            child.num_layers = max(1, min(6, child.num_layers + random.choice([-1, 1])))\n",
    "        if random.random() < 0.6:\n",
    "            idx = random.randrange(len(child.hidden_dims))\n",
    "            factor = random.choice([1, 2]) if random.random() < 0.8 else random.choice([0.5, 1])\n",
    "            child.hidden_dims[idx] = int(max(8, min(2048, int(child.hidden_dims[idx] * factor))))\n",
    "        if random.random() < 0.2:\n",
    "            child.activation = random.choice(['relu', 'selu', 'tanh', 'gelu'])\n",
    "        if random.random() < 0.3:\n",
    "            child.dropout = float(max(0.0, min(0.6, child.dropout + random.uniform(-0.15, 0.15))))\n",
    "        if random.random() < 0.2:\n",
    "            child.use_bn = not child.use_bn\n",
    "        if len(child.hidden_dims) < child.num_layers:\n",
    "            child.hidden_dims += [child.hidden_dims[-1]] * (child.num_layers - len(child.hidden_dims))\n",
    "        child.hidden_dims = child.hidden_dims[:child.num_layers]\n",
    "        return child\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ArchSpec(layers={self.num_layers}, dims={self.hidden_dims}, act={self.activation}, drop={self.dropout:.2f}, bn={self.use_bn})\"\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, spec: ArchSpec):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for i in range(spec.num_layers):\n",
    "            out_dim = spec.hidden_dims[i]\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if spec.use_bn:\n",
    "                layers.append(nn.BatchNorm1d(out_dim))\n",
    "            layers.append(self._get_activation(spec.activation))\n",
    "            if spec.dropout > 0:\n",
    "                layers.append(nn.Dropout(spec.dropout))\n",
    "            in_dim = out_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_activation(name: str):\n",
    "        if name == 'relu':\n",
    "            return nn.ReLU(inplace=True)\n",
    "        if name == 'selu':\n",
    "            return nn.SELU(inplace=True)\n",
    "        if name == 'tanh':\n",
    "            return nn.Tanh()\n",
    "        if name == 'gelu':\n",
    "            return nn.GELU()\n",
    "        return nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --------------------- Training/Eval loops ---------------------\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, device: torch.device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n = 0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        bs = xb.shape[0]\n",
    "        running_loss += loss.item() * bs\n",
    "        n += bs\n",
    "    return running_loss / max(1, n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model: nn.Module, loader: DataLoader, device: torch.device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        out = model(xb).cpu().numpy()\n",
    "        preds.append(out)\n",
    "        trues.append(yb.numpy())\n",
    "    if len(preds) == 0:\n",
    "        return np.zeros((0,)), np.zeros((0,))\n",
    "    preds = np.vstack(preds)\n",
    "    trues = np.vstack(trues)\n",
    "    return trues, preds\n",
    "\n",
    "# --------------------- NAS search ---------------------\n",
    "def random_arch_spec():\n",
    "    num_layers = random.choice([1, 2, 3, 4])\n",
    "    hidden_dims = [random.choice([32, 64, 128, 256, 512]) for _ in range(num_layers)]\n",
    "    activation = random.choice(['relu', 'selu', 'tanh', 'gelu'])\n",
    "    dropout = random.choice([0.0, 0.1, 0.2, 0.3])\n",
    "    use_bn = random.choice([True, False])\n",
    "    return ArchSpec(num_layers=num_layers, hidden_dims=hidden_dims, activation=activation, dropout=dropout, use_bn=use_bn)\n",
    "\n",
    "def search_architectures(train_loader: DataLoader, val_loader: DataLoader, input_dim: int, output_dim: int,\n",
    "                         device: torch.device, pop_size: int = 6, generations: int = 3, search_epochs: int = 5,\n",
    "                         lr: float = 1e-3):\n",
    "    population = [random_arch_spec() for _ in range(pop_size)]\n",
    "    best_spec = None\n",
    "    best_score = float('inf')  # lower val MSE better\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for gen in range(generations):\n",
    "        print(f\"\\n=== Generation {gen+1}/{generations} ===\")\n",
    "        scored = []\n",
    "        for i, spec in enumerate(population):\n",
    "            print(f\" Candidate {i+1}/{len(population)}: {spec}\")\n",
    "            model = MLP(input_dim, output_dim, spec).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            for ep in range(search_epochs):\n",
    "                tr_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "                trues, preds = evaluate_model(model, val_loader, device)\n",
    "                val_mse_avg = float(np.mean(mse_per_output(trues, preds))) if trues.size else float('inf')\n",
    "                print(f\"  ep {ep+1}/{search_epochs}  train_loss={tr_loss:.6e}  val_mse_avg={val_mse_avg:.6e}\")\n",
    "            trues, preds = evaluate_model(model, val_loader, device)\n",
    "            val_mse_avg = float(np.mean(mse_per_output(trues, preds))) if trues.size else float('inf')\n",
    "            scored.append((val_mse_avg, spec))\n",
    "            if val_mse_avg < best_score:\n",
    "                best_score = val_mse_avg\n",
    "                best_spec = deepcopy(spec)\n",
    "                print(f\"  -> New best (val_mse_avg={best_score:.6e}): {best_spec}\")\n",
    "        scored.sort(key=lambda x: x[0])\n",
    "        keep = [s for _, s in scored[:max(1, len(scored)//2)]]\n",
    "        new_pop = deepcopy(keep)\n",
    "        while len(new_pop) < pop_size:\n",
    "            parent = random.choice(keep)\n",
    "            new_pop.append(parent.mutate())\n",
    "        population = new_pop\n",
    "\n",
    "    print(f\"\\nSearch finished. Best spec: {best_spec} (val_mse_avg={best_score:.6e})\")\n",
    "    return best_spec\n",
    "\n",
    "# --------------------- Main ---------------------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"NAS for INS calibration (PyTorch only). Supports CSV or .pth inputs.\")\n",
    "    parser.add_argument('--train_csv', type=str, default=None, help='path to train CSV (optional)')\n",
    "    parser.add_argument('--test_csv', type=str, default=None, help='path to test CSV (optional)')\n",
    "    parser.add_argument('--train_pth', type=str, default=None, help='path to train .pth (optional; overrides CSV if given)')\n",
    "    parser.add_argument('--test_pth', type=str, default=None, help='path to test .pth (optional; overrides CSV if given)')\n",
    "    parser.add_argument('--input_cols', type=str, default=None, help='comma-separated input columns (optional)')\n",
    "    parser.add_argument('--target_cols', type=str, required=True, help='comma-separated target columns (e.g., bias_x,bias_y)')\n",
    "    parser.add_argument('--sep', type=str, default=',', help='CSV separator')\n",
    "    parser.add_argument('--pop_size', type=int, default=6)\n",
    "    parser.add_argument('--generations', type=int, default=3)\n",
    "    parser.add_argument('--search_epochs', type=int, default=6)\n",
    "    parser.add_argument('--final_epochs', type=int, default=40)\n",
    "    parser.add_argument('--batch_size', type=int, default=128)\n",
    "    parser.add_argument('--val_fraction', type=float, default=0.1)\n",
    "    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--save', type=str, default='nas_ins_best.pth')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    set_seed(args.seed)\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    # Load train data\n",
    "    if args.train_pth:\n",
    "        X_train_all, Y_train_all = load_pth_as_arrays(args.train_pth)\n",
    "        input_cols = None\n",
    "        inferred_input_cols = None\n",
    "    elif args.train_csv:\n",
    "        input_cols = None if args.input_cols is None else [c.strip() for c in args.input_cols.split(',') if c.strip()]\n",
    "        target_cols = [c.strip() for c in args.target_cols.split(',') if c.strip()]\n",
    "        X_train_all, Y_train_all, inferred_input_cols, _ = load_csv_as_arrays(args.train_csv, input_cols, target_cols, sep=args.sep)\n",
    "        if input_cols is None:\n",
    "            input_cols = inferred_input_cols\n",
    "    else:\n",
    "        raise ValueError(\"Provide either --train_pth or --train_csv\")\n",
    "\n",
    "    # Load test data\n",
    "    if args.test_pth:\n",
    "        X_test, Y_test = load_pth_as_arrays(args.test_pth)\n",
    "    elif args.test_csv:\n",
    "        # If input_cols was None earlier and inferred, use same input list for test\n",
    "        target_cols = [c.strip() for c in args.target_cols.split(',') if c.strip()]\n",
    "        X_test, Y_test, _, _ = load_csv_as_arrays(args.test_csv, input_cols, target_cols, sep=args.sep)\n",
    "    else:\n",
    "        raise ValueError(\"Provide either --test_pth or --test_csv\")\n",
    "\n",
    "    if X_train_all.shape[0] != Y_train_all.shape[0]:\n",
    "        raise ValueError(\"Train X and Y first dimensions disagree\")\n",
    "    if X_test.shape[0] != Y_test.shape[0]:\n",
    "        raise ValueError(\"Test X and Y first dimensions disagree\")\n",
    "\n",
    "    input_dim = X_train_all.shape[1]\n",
    "    output_dim = Y_train_all.shape[1]\n",
    "    print(f\"Input dim: {input_dim}, Output dim: {output_dim}\")\n",
    "    print(f\"Input cols (if known): {input_cols}\")\n",
    "    print(f\"Target cols: {args.target_cols}\")\n",
    "\n",
    "    # prepare train/val split for search\n",
    "    n_total = X_train_all.shape[0]\n",
    "    n_val = max(1, int(args.val_fraction * n_total))\n",
    "    n_train = n_total - n_val\n",
    "    perm = np.random.permutation(n_total)\n",
    "    X_sh = X_train_all[perm]\n",
    "    Y_sh = Y_train_all[perm]\n",
    "    X_train = X_sh[:n_train]\n",
    "    Y_train = Y_sh[:n_train]\n",
    "    X_val = X_sh[n_train:]\n",
    "    Y_val = Y_sh[n_train:]\n",
    "\n",
    "    # Fit scalers on train (search) split\n",
    "    x_scaler = Scaler()\n",
    "    x_scaler.fit(X_train)\n",
    "    X_train_s = x_scaler.transform(X_train)\n",
    "    X_val_s = x_scaler.transform(X_val)\n",
    "    X_test_s = x_scaler.transform(X_test)\n",
    "\n",
    "    y_scaler = Scaler()\n",
    "    y_scaler.fit(Y_train)\n",
    "    Y_train_s = y_scaler.transform(Y_train)\n",
    "    Y_val_s = y_scaler.transform(Y_val)\n",
    "    Y_test_s = y_scaler.transform(Y_test)  # used only as scaled reference if needed\n",
    "\n",
    "    train_loader = make_dataloader(X_train_s.astype(np.float32), Y_train_s.astype(np.float32), batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader = make_dataloader(X_val_s.astype(np.float32), Y_val_s.astype(np.float32), batch_size=args.batch_size, shuffle=False)\n",
    "    test_loader_scaled = make_dataloader(X_test_s.astype(np.float32), Y_test_s.astype(np.float32), batch_size=args.batch_size, shuffle=False)\n",
    "    # loader for final evaluation with original Y\n",
    "    test_loader_orig = make_dataloader(X_test_s.astype(np.float32), Y_test.astype(np.float32), batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # Search\n",
    "    best_spec = search_architectures(train_loader, val_loader, input_dim, output_dim, device,\n",
    "                                     pop_size=args.pop_size, generations=args.generations,\n",
    "                                     search_epochs=args.search_epochs, lr=args.lr)\n",
    "\n",
    "    # Retrain best on full training data (train + val)\n",
    "    X_full = np.vstack([X_train, X_val])\n",
    "    Y_full = np.vstack([Y_train, Y_val])\n",
    "    # fit scalers on full training\n",
    "    x_scaler_full = Scaler(); x_scaler_full.fit(X_full)\n",
    "    y_scaler_full = Scaler(); y_scaler_full.fit(Y_full)\n",
    "    X_full_s = x_scaler_full.transform(X_full)\n",
    "    Y_full_s = y_scaler_full.transform(Y_full)\n",
    "    X_test_s_full = x_scaler_full.transform(X_test)  # re-scale test using full-train scalers\n",
    "\n",
    "    full_loader = make_dataloader(X_full_s.astype(np.float32), Y_full_s.astype(np.float32), batch_size=args.batch_size, shuffle=True)\n",
    "    test_loader_final_scaled = make_dataloader(X_test_s_full.astype(np.float32), Y_test.astype(np.float32), batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    model = MLP(input_dim, output_dim, best_spec).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=max(10, args.final_epochs//3), gamma=0.5)\n",
    "\n",
    "    print(\"\\nRetraining best architecture on FULL training set...\")\n",
    "    for ep in range(1, args.final_epochs + 1):\n",
    "        tr_loss = train_one_epoch(model, full_loader, criterion, optimizer, device)\n",
    "        scheduler.step()\n",
    "        if ep % 5 == 0 or ep == 1 or ep == args.final_epochs:\n",
    "            # compute quick val on test scaled just for monitoring (not the final evaluation)\n",
    "            trues, preds_scaled = evaluate_model(model, test_loader_final_scaled, device)\n",
    "            # preds_scaled here are in original Y space because test_loader_final_scaled contains original Y (we used Y_test in loader), so we must be careful:\n",
    "            # Instead we'll compute scaled predictions by passing scaled inputs and then inverse-transform using y_scaler_full.\n",
    "            with torch.no_grad():\n",
    "                preds_s = []\n",
    "                for xb_s, _ in make_dataloader(X_test_s_full.astype(np.float32), Y_test_s.astype(np.float32), batch_size=args.batch_size, shuffle=False):\n",
    "                    preds_s.append(model(xb_s.to(device)).cpu().numpy())\n",
    "                preds_s = np.vstack(preds_s)\n",
    "            preds_unscaled = y_scaler_full.inverse_transform(preds_s)\n",
    "            val_mse = float(np.mean(mse_per_output(Y_test, preds_unscaled)))\n",
    "            print(f\" Final Ep {ep}/{args.final_epochs}  train_loss(scaled)={tr_loss:.6e}  test_val_mse={val_mse:.6e}\")\n",
    "\n",
    "    # Save checkpoint (model state + spec + scalers + columns)\n",
    "    save_obj = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'spec': best_spec.__dict__,\n",
    "        'x_scaler_mean': x_scaler_full.mean,\n",
    "        'x_scaler_std': x_scaler_full.std,\n",
    "        'y_scaler_mean': y_scaler_full.mean,\n",
    "        'y_scaler_std': y_scaler_full.std,\n",
    "        'input_cols': input_cols,\n",
    "        'target_cols': [c.strip() for c in args.target_cols.split(',') if c.strip()]\n",
    "    }\n",
    "    torch.save(save_obj, args.save)\n",
    "    print(f\"\\nSaved checkpoint to {args.save}\")\n",
    "\n",
    "    # Final evaluation on test dataset (original scale)\n",
    "    print(\"\\nFinal evaluation on TEST dataset (original scale):\")\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for xb_s, y_orig in test_loader_orig:\n",
    "            xb_s = xb_s.to(device)\n",
    "            out_scaled = model(xb_s).cpu().numpy()\n",
    "            out_unscaled = y_scaler_full.inverse_transform(out_scaled)\n",
    "            preds.append(out_unscaled)\n",
    "            trues.append(y_orig.numpy())\n",
    "    preds = np.vstack(preds)\n",
    "    trues = np.vstack(trues)\n",
    "    print_metrics(\"TEST\", trues, preds, save_obj['target_cols'])\n",
    "\n",
    "    # show first few predictions\n",
    "    n_show = min(8, preds.shape[0])\n",
    "    print(f\"First {n_show} predictions vs truth (per-target):\")\n",
    "    for i in range(n_show):\n",
    "        pred_str = \", \".join(f\"{v:.6e}\" for v in preds[i])\n",
    "        true_str = \", \".join(f\"{v:.6e}\" for v in trues[i])\n",
    "        print(f\"  row {i+1:03d}: pred = [{pred_str}]  |  true = [{true_str}]\")\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
