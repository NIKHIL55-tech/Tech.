{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9d7c47",
   "metadata": {},
   "source": [
    "ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# ===============================\n",
    "# 1. Load Dataset\n",
    "# ===============================\n",
    "df = pd.read_csv(\"your_dataset.csv\")  # <<< PUT YOUR FILE HERE\n",
    "\n",
    "target_col = \"your_target_column\"  # <<< PUT YOUR TARGET COLUMN NAME\n",
    "feature_cols = [c for c in df.columns if c != target_col]\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[target_col].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# ===============================\n",
    "# 2. Define Neural Net Class\n",
    "# ===============================\n",
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, hidden_units, dropout, output_dim=1):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(prev_dim, hidden_units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_units\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ===============================\n",
    "# 3. Training & Evaluation\n",
    "# ===============================\n",
    "def train_and_eval(hidden_layers, hidden_units, dropout, lr, epochs=40):\n",
    "    model = DynamicNet(X_train.shape[1], hidden_layers, hidden_units, dropout)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()  # Change to BCEWithLogitsLoss if classification\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_train)\n",
    "        loss = loss_fn(preds, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_test)\n",
    "        val_loss = loss_fn(preds, y_test).item()\n",
    "    return val_loss, model\n",
    "\n",
    "# ===============================\n",
    "# 4. Search Space for NAS\n",
    "# ===============================\n",
    "space = [\n",
    "    Integer(1, 4, name=\"hidden_layers\"),         # number of hidden layers\n",
    "    Integer(16, 256, name=\"hidden_units\"),       # neurons per layer\n",
    "    Real(0.0, 0.5, name=\"dropout\"),              # dropout rate\n",
    "    Real(1e-4, 1e-2, prior=\"log-uniform\", name=\"lr\")  # learning rate\n",
    "]\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(hidden_layers, hidden_units, dropout, lr):\n",
    "    val_loss, _ = train_and_eval(hidden_layers, hidden_units, dropout, lr)\n",
    "    return val_loss\n",
    "\n",
    "# ===============================\n",
    "# 5. Run Bayesian Optimization\n",
    "# ===============================\n",
    "print(\"üîé Running Neural Architecture Search...\")\n",
    "result = gp_minimize(objective, space, n_calls=20, random_state=42)\n",
    "\n",
    "best_hl, best_hu, best_do, best_lr = result.x\n",
    "print(f\"\\n‚úÖ Best NAS config -> Layers: {best_hl}, Units: {best_hu}, Dropout: {best_do:.2f}, LR: {best_lr:.5f}\")\n",
    "\n",
    "# ===============================\n",
    "# 6. Train Final Model\n",
    "# ===============================\n",
    "final_loss, best_model = train_and_eval(best_hl, best_hu, best_do, best_lr, epochs=100)\n",
    "print(f\"üî• Final Model Validation Loss: {final_loss:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# 7. Save the Model\n",
    "# ===============================\n",
    "torch.save(best_model.state_dict(), \"best_nas_model.pth\")\n",
    "print(\"üì¶ Best model saved as best_nas_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e6865",
   "metadata": {},
   "source": [
    "Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f2493",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Architecture Configuration\n",
    "@dataclass\n",
    "class ArchConfig:\n",
    "    \"\"\"Configuration for a neural architecture\"\"\"\n",
    "    sequence_length: int\n",
    "    input_features: int\n",
    "    hidden_dims: List[int]\n",
    "    layer_types: List[str]  # ['lstm', 'gru', 'conv1d', 'linear']\n",
    "    attention_heads: Optional[int]\n",
    "    dropout_rates: List[float]\n",
    "    use_skip_connections: bool\n",
    "    activation: str  # 'relu', 'tanh', 'gelu'\n",
    "    output_dim: int\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Multi-head attention for time series\"\"\"\n",
    "    def __init__(self, input_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.input_dim = input_dim\n",
    "        self.head_dim = input_dim // num_heads\n",
    "        \n",
    "        assert input_dim % num_heads == 0, \"input_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.output = nn.Linear(input_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        context = torch.matmul(attention, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.input_dim)\n",
    "        \n",
    "        return self.output(context)\n",
    "\n",
    "class ArchitectureSearchSpace:\n",
    "    \"\"\"Defines the search space for NAS\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_search_space():\n",
    "        return {\n",
    "            'hidden_dims': [\n",
    "                [64, 32],\n",
    "                [128, 64],\n",
    "                [256, 128],\n",
    "                [128, 128, 64],\n",
    "                [256, 128, 64],\n",
    "                [512, 256, 128],\n",
    "                [256, 256, 128, 64]\n",
    "            ],\n",
    "            'layer_types': [\n",
    "                ['lstm', 'linear'],\n",
    "                ['gru', 'linear'],\n",
    "                ['conv1d', 'lstm', 'linear'],\n",
    "                ['lstm', 'lstm', 'linear'],\n",
    "                ['conv1d', 'gru', 'linear'],\n",
    "                ['lstm', 'attention', 'linear'],\n",
    "                ['conv1d', 'lstm', 'attention', 'linear']\n",
    "            ],\n",
    "            'attention_heads': [None, 4, 8, 16],\n",
    "            'dropout_rates': [\n",
    "                [0.1, 0.1],\n",
    "                [0.2, 0.2],\n",
    "                [0.3, 0.1],\n",
    "                [0.1, 0.2, 0.1],\n",
    "                [0.2, 0.3, 0.1],\n",
    "                [0.1, 0.1, 0.2, 0.1]\n",
    "            ],\n",
    "            'use_skip_connections': [True, False],\n",
    "            'activation': ['relu', 'tanh', 'gelu']\n",
    "        }\n",
    "\n",
    "class DynamicINSModel(nn.Module):\n",
    "    \"\"\"Dynamic model that can be configured based on architecture config\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ArchConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.skip_connections = []\n",
    "        \n",
    "        # Build the architecture\n",
    "        current_dim = config.input_features\n",
    "        \n",
    "        for i, (layer_type, hidden_dim, dropout_rate) in enumerate(\n",
    "            zip(config.layer_types, config.hidden_dims, config.dropout_rates)\n",
    "        ):\n",
    "            if layer_type == 'lstm':\n",
    "                layer = nn.LSTM(\n",
    "                    input_size=current_dim,\n",
    "                    hidden_size=hidden_dim,\n",
    "                    batch_first=True,\n",
    "                    dropout=dropout_rate if i < len(config.layer_types) - 1 else 0\n",
    "                )\n",
    "                self.layers.append(layer)\n",
    "                current_dim = hidden_dim\n",
    "                \n",
    "            elif layer_type == 'gru':\n",
    "                layer = nn.GRU(\n",
    "                    input_size=current_dim,\n",
    "                    hidden_size=hidden_dim,\n",
    "                    batch_first=True,\n",
    "                    dropout=dropout_rate if i < len(config.layer_types) - 1 else 0\n",
    "                )\n",
    "                self.layers.append(layer)\n",
    "                current_dim = hidden_dim\n",
    "                \n",
    "            elif layer_type == 'conv1d':\n",
    "                layer = nn.Sequential(\n",
    "                    nn.Conv1d(current_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "                    self._get_activation(),\n",
    "                    nn.Dropout(dropout_rate)\n",
    "                )\n",
    "                self.layers.append(layer)\n",
    "                current_dim = hidden_dim\n",
    "                \n",
    "            elif layer_type == 'attention':\n",
    "                if config.attention_heads:\n",
    "                    # Ensure current_dim is divisible by attention_heads\n",
    "                    if current_dim % config.attention_heads != 0:\n",
    "                        # Adjust current_dim to be divisible\n",
    "                        adjustment_layer = nn.Linear(current_dim, \n",
    "                                                   (current_dim // config.attention_heads) * config.attention_heads)\n",
    "                        self.layers.append(adjustment_layer)\n",
    "                        current_dim = (current_dim // config.attention_heads) * config.attention_heads\n",
    "                    \n",
    "                    layer = AttentionLayer(current_dim, config.attention_heads)\n",
    "                    self.layers.append(layer)\n",
    "                    \n",
    "            elif layer_type == 'linear':\n",
    "                layer = nn.Sequential(\n",
    "                    nn.Linear(current_dim, hidden_dim),\n",
    "                    self._get_activation(),\n",
    "                    nn.Dropout(dropout_rate)\n",
    "                )\n",
    "                self.layers.append(layer)\n",
    "                current_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(current_dim, config.output_dim)\n",
    "        \n",
    "        # Skip connection compatibility\n",
    "        if config.use_skip_connections:\n",
    "            self.skip_projections = nn.ModuleList()\n",
    "            skip_dim = config.input_features\n",
    "            for hidden_dim in config.hidden_dims[:-1]:  # Skip the last layer for output\n",
    "                if skip_dim != hidden_dim:\n",
    "                    self.skip_projections.append(nn.Linear(skip_dim, hidden_dim))\n",
    "                else:\n",
    "                    self.skip_projections.append(nn.Identity())\n",
    "                skip_dim = hidden_dim\n",
    "        \n",
    "    def _get_activation(self):\n",
    "        if self.config.activation == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif self.config.activation == 'tanh':\n",
    "            return nn.Tanh()\n",
    "        elif self.config.activation == 'gelu':\n",
    "            return nn.GELU()\n",
    "        else:\n",
    "            return nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_features)\n",
    "        original_x = x\n",
    "        skip_x = x\n",
    "        \n",
    "        for i, (layer, layer_type) in enumerate(zip(self.layers, self.config.layer_types)):\n",
    "            if layer_type in ['lstm', 'gru']:\n",
    "                x, _ = layer(x)\n",
    "                # Take the last output for sequence prediction\n",
    "                if i == len(self.layers) - 1 or self.config.layer_types[i + 1] == 'linear':\n",
    "                    x = x[:, -1, :]  # Take last time step\n",
    "                    \n",
    "            elif layer_type == 'conv1d':\n",
    "                # Conv1d expects (batch_size, features, sequence_length)\n",
    "                x = x.transpose(1, 2)\n",
    "                x = layer(x)\n",
    "                x = x.transpose(1, 2)\n",
    "                \n",
    "            elif layer_type == 'attention':\n",
    "                x = layer(x)\n",
    "                \n",
    "            elif layer_type == 'linear':\n",
    "                # If x is still 3D, take the last time step\n",
    "                if len(x.shape) == 3:\n",
    "                    x = x[:, -1, :]\n",
    "                x = layer(x)\n",
    "            \n",
    "            # Skip connections (only for compatible dimensions)\n",
    "            if (self.config.use_skip_connections and \n",
    "                i < len(self.skip_projections) and \n",
    "                len(skip_x.shape) == len(x.shape)):\n",
    "                try:\n",
    "                    if len(x.shape) == 3:  # Sequence data\n",
    "                        projected_skip = self.skip_projections[i](skip_x)\n",
    "                        if projected_skip.shape == x.shape:\n",
    "                            x = x + projected_skip\n",
    "                    elif len(x.shape) == 2:  # After taking last time step\n",
    "                        if len(skip_x.shape) == 3:\n",
    "                            skip_x = skip_x[:, -1, :]\n",
    "                        projected_skip = self.skip_projections[i](skip_x)\n",
    "                        if projected_skip.shape == x.shape:\n",
    "                            x = x + projected_skip\n",
    "                    skip_x = x\n",
    "                except:\n",
    "                    # Skip connection failed, continue without it\n",
    "                    pass\n",
    "        \n",
    "        # Final output\n",
    "        if len(x.shape) == 3:\n",
    "            x = x[:, -1, :]\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "\n",
    "class INSDataset(Dataset):\n",
    "    \"\"\"Dataset class for INS data\"\"\"\n",
    "    \n",
    "    def __init__(self, data, target_column, feature_columns, sequence_length, scaler_X=None, scaler_y=None):\n",
    "        self.data = data\n",
    "        self.target_column = target_column\n",
    "        self.feature_columns = feature_columns\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Prepare features and targets\n",
    "        X = data[feature_columns].values\n",
    "        y = data[target_column].values\n",
    "        \n",
    "        # Scale the data\n",
    "        if scaler_X is None:\n",
    "            self.scaler_X = StandardScaler()\n",
    "            X = self.scaler_X.fit_transform(X)\n",
    "        else:\n",
    "            self.scaler_X = scaler_X\n",
    "            X = self.scaler_X.transform(X)\n",
    "            \n",
    "        if scaler_y is None:\n",
    "            self.scaler_y = StandardScaler()\n",
    "            y = self.scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "        else:\n",
    "            self.scaler_y = scaler_y\n",
    "            y = self.scaler_y.transform(y.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Create sequences\n",
    "        self.sequences, self.targets = self._create_sequences(X, y)\n",
    "        \n",
    "    def _create_sequences(self, X, y):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in range(len(X) - self.sequence_length + 1):\n",
    "            seq = X[i:i + self.sequence_length]\n",
    "            target = y[i + self.sequence_length - 1]  # Predict the last point in sequence\n",
    "            sequences.append(seq)\n",
    "            targets.append(target)\n",
    "            \n",
    "        return torch.FloatTensor(sequences), torch.FloatTensor(targets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "\n",
    "class NASController:\n",
    "    \"\"\"Neural Architecture Search Controller\"\"\"\n",
    "    \n",
    "    def __init__(self, input_features, sequence_length, output_dim=1, device='cuda'):\n",
    "        self.input_features = input_features\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "        self.device = device\n",
    "        self.search_space = ArchitectureSearchSpace.get_search_space()\n",
    "        self.architecture_history = []\n",
    "        self.performance_history = []\n",
    "        \n",
    "    def sample_architecture(self) -> ArchConfig:\n",
    "        \"\"\"Sample a random architecture from the search space\"\"\"\n",
    "        \n",
    "        # Sample basic components\n",
    "        layer_types = random.choice(self.search_space['layer_types'])\n",
    "        hidden_dims = random.choice([dims for dims in self.search_space['hidden_dims'] \n",
    "                                   if len(dims) == len(layer_types)])\n",
    "        \n",
    "        # Ensure dropout rates match the number of layers\n",
    "        dropout_rates = random.choice([rates for rates in self.search_space['dropout_rates']\n",
    "                                     if len(rates) == len(layer_types)])\n",
    "        \n",
    "        # Sample other hyperparameters\n",
    "        attention_heads = random.choice(self.search_space['attention_heads'])\n",
    "        use_skip_connections = random.choice(self.search_space['use_skip_connections'])\n",
    "        activation = random.choice(self.search_space['activation'])\n",
    "        \n",
    "        return ArchConfig(\n",
    "            sequence_length=self.sequence_length,\n",
    "            input_features=self.input_features,\n",
    "            hidden_dims=hidden_dims,\n",
    "            layer_types=layer_types,\n",
    "            attention_heads=attention_heads,\n",
    "            dropout_rates=dropout_rates,\n",
    "            use_skip_connections=use_skip_connections,\n",
    "            activation=activation,\n",
    "            output_dim=self.output_dim\n",
    "        )\n",
    "    \n",
    "    def evaluate_architecture(self, config: ArchConfig, train_loader, val_loader, \n",
    "                            epochs=20, patience=5) -> Dict:\n",
    "        \"\"\"Evaluate a single architecture\"\"\"\n",
    "        \n",
    "        model = DynamicINSModel(config).to(self.device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs.squeeze(), batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_batches += 1\n",
    "            \n",
    "            avg_train_loss = train_loss / train_batches\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_batches = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                    outputs = model(batch_x)\n",
    "                    loss = criterion(outputs.squeeze(), batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches\n",
    "            \n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate model parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        # Calculate inference time\n",
    "        model.eval()\n",
    "        inference_times = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):  # Average over 10 runs\n",
    "                batch_x = next(iter(val_loader))[0][:1].to(self.device)  # Single sample\n",
    "                start_inf = time.time()\n",
    "                _ = model(batch_x)\n",
    "                inference_times.append(time.time() - start_inf)\n",
    "        \n",
    "        avg_inference_time = np.mean(inference_times) * 1000  # Convert to ms\n",
    "        \n",
    "        results = {\n",
    "            'val_loss': best_val_loss,\n",
    "            'train_loss': train_losses[-1],\n",
    "            'training_time': training_time,\n",
    "            'inference_time_ms': avg_inference_time,\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params,\n",
    "            'epochs_trained': epoch + 1,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def search(self, train_loader, val_loader, num_trials=50, epochs_per_trial=20):\n",
    "        \"\"\"Main search loop\"\"\"\n",
    "        \n",
    "        print(f\"Starting NAS with {num_trials} trials...\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Input features: {self.input_features}\")\n",
    "        print(f\"Sequence length: {self.sequence_length}\")\n",
    "        \n",
    "        best_architectures = []\n",
    "        \n",
    "        for trial in range(num_trials):\n",
    "            print(f\"\\n--- Trial {trial + 1}/{num_trials} ---\")\n",
    "            \n",
    "            # Sample architecture\n",
    "            config = self.sample_architecture()\n",
    "            \n",
    "            print(f\"Architecture: {config.layer_types}\")\n",
    "            print(f\"Hidden dims: {config.hidden_dims}\")\n",
    "            print(f\"Attention heads: {config.attention_heads}\")\n",
    "            print(f\"Skip connections: {config.use_skip_connections}\")\n",
    "            \n",
    "            try:\n",
    "                # Evaluate architecture\n",
    "                results = self.evaluate_architecture(\n",
    "                    config, train_loader, val_loader, \n",
    "                    epochs=epochs_per_trial\n",
    "                )\n",
    "                \n",
    "                self.architecture_history.append(config)\n",
    "                self.performance_history.append(results)\n",
    "                \n",
    "                print(f\"Val Loss: {results['val_loss']:.4f}\")\n",
    "                print(f\"Params: {results['total_params']:,}\")\n",
    "                print(f\"Inference time: {results['inference_time_ms']:.2f}ms\")\n",
    "                print(f\"Training time: {results['training_time']:.1f}s\")\n",
    "                \n",
    "                # Keep track of best architectures\n",
    "                best_architectures.append(results)\n",
    "                best_architectures.sort(key=lambda x: x['val_loss'])\n",
    "                best_architectures = best_architectures[:10]  # Keep top 10\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Trial {trial + 1} failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Sort results\n",
    "        self.performance_history.sort(key=lambda x: x['val_loss'])\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"SEARCH COMPLETED!\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Display top 5 architectures\n",
    "        for i, result in enumerate(self.performance_history[:5]):\n",
    "            config = result['config']\n",
    "            print(f\"\\nRank {i+1}:\")\n",
    "            print(f\"  Architecture: {config.layer_types}\")\n",
    "            print(f\"  Hidden dims: {config.hidden_dims}\")\n",
    "            print(f\"  Validation Loss: {result['val_loss']:.4f}\")\n",
    "            print(f\"  Parameters: {result['total_params']:,}\")\n",
    "            print(f\"  Inference time: {result['inference_time_ms']:.2f}ms\")\n",
    "        \n",
    "        return self.performance_history\n",
    "    \n",
    "    def get_best_architecture(self):\n",
    "        \"\"\"Get the best performing architecture\"\"\"\n",
    "        if not self.performance_history:\n",
    "            return None\n",
    "        return self.performance_history[0]\n",
    "    \n",
    "    def save_results(self, filename):\n",
    "        \"\"\"Save search results\"\"\"\n",
    "        # Convert results to serializable format\n",
    "        serializable_results = []\n",
    "        for result in self.performance_history:\n",
    "            config = result['config']\n",
    "            serializable_result = {\n",
    "                'val_loss': result['val_loss'],\n",
    "                'train_loss': result['train_loss'],\n",
    "                'training_time': result['training_time'],\n",
    "                'inference_time_ms': result['inference_time_ms'],\n",
    "                'total_params': result['total_params'],\n",
    "                'trainable_params': result['trainable_params'],\n",
    "                'epochs_trained': result['epochs_trained'],\n",
    "                'config': {\n",
    "                    'sequence_length': config.sequence_length,\n",
    "                    'input_features': config.input_features,\n",
    "                    'hidden_dims': config.hidden_dims,\n",
    "                    'layer_types': config.layer_types,\n",
    "                    'attention_heads': config.attention_heads,\n",
    "                    'dropout_rates': config.dropout_rates,\n",
    "                    'use_skip_connections': config.use_skip_connections,\n",
    "                    'activation': config.activation,\n",
    "                    'output_dim': config.output_dim\n",
    "                }\n",
    "            }\n",
    "            serializable_results.append(serializable_result)\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "# ====================================================================\n",
    "# USER CONFIGURATION SECTION - MODIFY THIS FOR YOUR DATA\n",
    "# ====================================================================\n",
    "\n",
    "def configure_your_data():\n",
    "    \"\"\"\n",
    "    Configure your dataset here. Replace this function with your actual data loading.\n",
    "    \n",
    "    Returns:\n",
    "        data (pd.DataFrame): Your dataset\n",
    "        feature_columns (list): List of column names to use as input features\n",
    "        target_column (str): Name of the column you want to predict\n",
    "    \"\"\"\n",
    "    \n",
    "    # METHOD 1: Load from CSV file\n",
    "    # Uncomment and modify the following lines for CSV loading:\n",
    "    \n",
    "    CSV_FILE_PATH = \"your_dataset.csv\"  # Replace with your CSV file path\n",
    "    TARGET_COLUMN = \"your_target_column\"  # Replace with your target column name\n",
    "    \n",
    "    # Option A: Specify feature columns manually\n",
    "    FEATURE_COLUMNS = [\n",
    "        \"feature1\", \"feature2\", \"feature3\",\n",
    "        \"feature4\", \"feature5\", \"feature6\", \n",
    "        \"feature7\", \"feature8\", \"feature9\"\n",
    "    ]\n",
    "    \n",
    "    # Option B: Use all columns except target as features (uncomment next line)\n",
    "    # FEATURE_COLUMNS = None  # Will auto-detect all columns except target\n",
    "    \n",
    "    data = pd.read_csv(CSV_FILE_PATH)\n",
    "    \n",
    "    if FEATURE_COLUMNS is None:\n",
    "        feature_columns = [col for col in data.columns if col != TARGET_COLUMN]\n",
    "    else:\n",
    "        feature_columns = FEATURE_COLUMNS\n",
    "    \n",
    "    return data, feature_columns, TARGET_COLUMN\n",
    "    \n",
    "    # METHOD 2: Load from DataFrame (if you already have data in memory)\n",
    "    # Uncomment and modify the following lines:\n",
    "    \n",
    "    # # Assume you have your DataFrame ready\n",
    "    # data = your_existing_dataframe\n",
    "    # \n",
    "    # TARGET_COLUMN = \"your_target_column\"\n",
    "    # FEATURE_COLUMNS = [\"col1\", \"col2\", \"col3\", \"col4\", \"col5\"]  # Your feature columns\n",
    "    # \n",
    "    # return data, FEATURE_COLUMNS, TARGET_COLUMN\n",
    "    \n",
    "    # METHOD 3: Manual data input (for small datasets or testing)\n",
    "    # Uncomment and modify:\n",
    "    \n",
    "    # # Create DataFrame manually\n",
    "    # data_dict = {\n",
    "    #     'feature1': [1, 2, 3, 4, 5, ...],  # Your actual data\n",
    "    #     'feature2': [1.1, 2.2, 3.3, 4.4, 5.5, ...],\n",
    "    #     'feature3': [0.1, 0.2, 0.3, 0.4, 0.5, ...],\n",
    "    #     'target': [10, 20, 30, 40, 50, ...]  # Your target values\n",
    "    # }\n",
    "    # \n",
    "    # data = pd.DataFrame(data_dict)\n",
    "    # feature_columns = ['feature1', 'feature2', 'feature3']\n",
    "    # target_column = 'target'\n",
    "    # \n",
    "    # return data, feature_columns, target_column\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# HYPERPARAMETER CONFIGURATION\n",
    "# ====================================================================\n",
    "\n",
    "def get_training_config():\n",
    "    \"\"\"\n",
    "    Configure training hyperparameters here.\n",
    "    Modify these values based on your dataset and computational resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    config = {\n",
    "        # Data parameters\n",
    "        'SEQUENCE_LENGTH': 50,  # How many time steps to look back for prediction\n",
    "        'BATCH_SIZE': 32,       # Training batch size (reduce if memory issues)\n",
    "        \n",
    "        # NAS parameters\n",
    "        'NUM_TRIALS': 25,        # Number of architectures to try (increase for better results)\n",
    "        'EPOCHS_PER_TRIAL': 15,  # Training epochs per architecture (balance speed vs accuracy)\n",
    "        \n",
    "        # Training parameters\n",
    "        'FINAL_TRAINING_EPOCHS': 100,  # Epochs for final best model training\n",
    "        'LEARNING_RATE': 0.001,        # Initial learning rate\n",
    "        'PATIENCE': 10,                # Early stopping patience\n",
    "        \n",
    "        # Data split\n",
    "        'TRAIN_SPLIT': 0.7,      # Training data ratio\n",
    "        'VALIDATION_SPLIT': 0.2, # Validation data ratio\n",
    "        'TEST_SPLIT': 0.1,       # Test data ratio (remaining)\n",
    "        \n",
    "        # Advanced options\n",
    "        'USE_EARLY_STOPPING': True,\n",
    "        'USE_GRADIENT_CLIPPING': True,\n",
    "        'GRADIENT_CLIP_VALUE': 1.0,\n",
    "        'USE_SCHEDULER': True,\n",
    "        'SCHEDULER_PATIENCE': 5,\n",
    "        \n",
    "        # Ensemble options\n",
    "        'CREATE_ENSEMBLE': True,\n",
    "        'ENSEMBLE_TOP_K': 3,  # Number of best models to ensemble,\n",
    "        \n",
    "        # Your specific configuration\n",
    "        'YOUR_TARGET_COLUMN': 'your_target_column',  # Set this to your target column name\n",
    "        'YOUR_FEATURE_COLUMNS': ['feature1', 'feature2', 'feature3']  # Set this to your feature columns\n",
    "    }\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# MAIN EXECUTION FUNCTION\n",
    "# ====================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run NAS for INS data\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"NEURAL ARCHITECTURE SEARCH FOR INS DATA PREDICTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Get configuration\n",
    "    config = get_training_config()\n",
    "    print(f\"\\nTraining Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Load your data\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"LOADING DATA\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    try:\n",
    "        data, feature_columns, target_column = configure_your_data()\n",
    "        \n",
    "        # Override with your specific configuration if provided\n",
    "        if 'YOUR_TARGET_COLUMN' in config and config['YOUR_TARGET_COLUMN'] != 'your_target_column':\n",
    "            target_column = config['YOUR_TARGET_COLUMN']\n",
    "        if 'YOUR_FEATURE_COLUMNS' in config and config['YOUR_FEATURE_COLUMNS'] != ['feature1', 'feature2', 'feature3']:\n",
    "            feature_columns = config['YOUR_FEATURE_COLUMNS']\n",
    "        \n",
    "        print(f\"‚úÖ Data loaded successfully!\")\n",
    "        print(f\"üìä Dataset shape: {data.shape}\")\n",
    "        print(f\"üéØ Target column: '{target_column}'\")\n",
    "        print(f\"üìà Feature columns ({len(feature_columns)}): {feature_columns}\")\n",
    "        \n",
    "        # Display first few rows\n",
    "        print(f\"\\nüìã First 5 rows of data:\")\n",
    "        print(data[feature_columns + [target_column]].head())\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_values = data[feature_columns + [target_column]].isnull().sum()\n",
    "        if missing_values.any():\n",
    "            print(f\"‚ö†Ô∏è  Missing values detected:\")\n",
    "            print(missing_values[missing_values > 0])\n",
    "            print(\"Filling missing values with forward fill...\")\n",
    "            data = data.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(f\"\\nüìä Data Statistics:\")\n",
    "        print(data[feature_columns + [target_column]].describe())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        print(\"Please check your data configuration in the configure_your_data() function.\")\n",
    "        return\n",
    "    \n",
    "    # Split data temporally (important for time series)\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"PREPARING DATA\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    n_samples = len(data)\n",
    "    train_end = int(config['TRAIN_SPLIT'] * n_samples)\n",
    "    val_end = int((config['TRAIN_SPLIT'] + config['VALIDATION_SPLIT']) * n_samples)\n",
    "    \n",
    "    train_data = data[:train_end]\n",
    "    val_data = data[train_end:val_end] \n",
    "    test_data = data[val_end:]\n",
    "    \n",
    "    print(f\"üìä Data splits:\")\n",
    "    print(f\"  Training: {len(train_data)} samples ({len(train_data)/n_samples*100:.1f}%)\")\n",
    "    print(f\"  Validation: {len(val_data)} samples ({len(val_data)/n_samples*100:.1f}%)\")\n",
    "    print(f\"  Test: {len(test_data)} samples ({len(test_data)/n_samples*100:.1f}%)\")\n",
    "    \n",
    "    if len(test_data) == 0:\n",
    "        print(\"‚ö†Ô∏è  Warning: No test data available. Using validation data for final testing.\")\n",
    "        test_data = val_data\n",
    "    \n",
    "    # Create datasets\n",
    "    print(f\"üîß Creating datasets with sequence length: {config['SEQUENCE_LENGTH']}\")\n",
    "    \n",
    "    try:\n",
    "        train_dataset = INSDataset(train_data, target_column, feature_columns, config['SEQUENCE_LENGTH'])\n",
    "        \n",
    "        # Use the same scalers for validation and test data\n",
    "        val_dataset = INSDataset(val_data, target_column, feature_columns, config['SEQUENCE_LENGTH'],\n",
    "                                scaler_X=train_dataset.scaler_X, scaler_y=train_dataset.scaler_y)\n",
    "        \n",
    "        test_dataset = INSDataset(test_data, target_column, feature_columns, config['SEQUENCE_LENGTH'],\n",
    "                                 scaler_X=train_dataset.scaler_X, scaler_y=train_dataset.scaler_y)\n",
    "        \n",
    "        print(f\"‚úÖ Datasets created successfully!\")\n",
    "        print(f\"  Train sequences: {len(train_dataset)}\")\n",
    "        print(f\"  Validation sequences: {len(val_dataset)}\")\n",
    "        print(f\"  Test sequences: {len(test_dataset)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating datasets: {e}\")\n",
    "        print(\"This might be due to insufficient data for the specified sequence length.\")\n",
    "        print(f\"Try reducing SEQUENCE_LENGTH (currently {config['SEQUENCE_LENGTH']}) in get_training_config().\")\n",
    "        return\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"üîß Data loaders created with batch size: {config['BATCH_SIZE']}\")\n",
    "    \n",
    "    # Initialize NAS controller\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"INITIALIZING NEURAL ARCHITECTURE SEARCH\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    nas_controller = NASController(\n",
    "        input_features=len(feature_columns),\n",
    "        sequence_length=config['SEQUENCE_LENGTH'],\n",
    "        output_dim=1,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(f\"üß† NAS Controller initialized\")\n",
    "    print(f\"  Input features: {len(feature_columns)}\")\n",
    "    print(f\"  Sequence length: {config['SEQUENCE_LENGTH']}\")\n",
    "    print(f\"  Search trials: {config['NUM_TRIALS']}\")\n",
    "    print(f\"  Epochs per trial: {config['EPOCHS_PER_TRIAL']}\")\n",
    "    \n",
    "    # Run search\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"RUNNING ARCHITECTURE SEARCH\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    results = nas_controller.search(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_trials=config['NUM_TRIALS'],\n",
    "        epochs_per_trial=config['EPOCHS_PER_TRIAL']\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_filename = f'nas_results_{timestamp}.json'\n",
    "    nas_controller.save_results(results_filename)\n",
    "    print(f\"\\nüíæ Results saved to '{results_filename}'\")\n",
    "    \n",
    "    # Get and train the best architecture\n",
    "    best_result = nas_controller.get_best_architecture()\n",
    "    if not best_result:\n",
    "        print(\"‚ùå No valid architectures found. Please check your data and try again.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"TRAINING BEST ARCHITECTURE\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    best_config = best_result['config']\n",
    "    print(f\"üèÜ Best architecture found:\")\n",
    "    print(f\"  Layer types: {best_config.layer_types}\")\n",
    "    print(f\"  Hidden dims: {best_config.hidden_dims}\")\n",
    "    print(f\"  Validation loss: {best_result['val_loss']:.4f}\")\n",
    "    print(f\"  Parameters: {best_result['total_params']:,}\")\n",
    "    \n",
    "    # Train the best model for more epochs\n",
    "    print(f\"\\nüöÄ Training for {config['FINAL_TRAINING_EPOCHS']} epochs...\")\n",
    "    \n",
    "    best_model = DynamicINSModel(best_config).to(device)\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=config['LEARNING_RATE'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if config['USE_SCHEDULER']:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, patience=config['SCHEDULER_PATIENCE']\n",
    "        )\n",
    "    \n",
    "    # Extended training\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(config['FINAL_TRAINING_EPOCHS']):\n",
    "        # Training phase\n",
    "        best_model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = best_model(batch_x)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            if config['USE_GRADIENT_CLIPPING']:\n",
    "                torch.nn.utils.clip_grad_norm_(best_model.parameters(), \n",
    "                                             max_norm=config['GRADIENT_CLIP_VALUE'])\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / train_batches\n",
    "        \n",
    "        # Validation phase\n",
    "        best_model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = best_model(batch_x)\n",
    "                loss = criterion(outputs.squeeze(), batch_y)\n",
    "                epoch_val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_val_loss = epoch_val_loss / val_batches\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        if config['USE_SCHEDULER']:\n",
    "            scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Early stopping and model saving\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model_filename = f'best_ins_model_{timestamp}.pth'\n",
    "            torch.save(best_model.state_dict(), model_filename)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1:3d}/{config['FINAL_TRAINING_EPOCHS']}] - \"\n",
    "                  f\"Train: {avg_train_loss:.4f}, Val: {avg_val_loss:.4f}, \"\n",
    "                  f\"Best Val: {best_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if config['USE_EARLY_STOPPING'] and patience_counter >= config['PATIENCE']:\n",
    "            print(f\"\\n‚èπÔ∏è  Early stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training completed!\")\n",
    "    print(f\"üíæ Best model saved as '{model_filename}'\")\n",
    "    print(f\"üèÜ Final best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Model evaluation on test set\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"FINAL MODEL EVALUATION\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    best_model.eval()\n",
    "    test_predictions = []\n",
    "    test_actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = best_model(batch_x)\n",
    "            \n",
    "            # Inverse transform predictions and actuals\n",
    "            pred_np = outputs.cpu().numpy().reshape(-1, 1)\n",
    "            actual_np = batch_y.cpu().numpy().reshape(-1, 1)\n",
    "            \n",
    "            pred_original = test_dataset.scaler_y.inverse_transform(pred_np).flatten()\n",
    "            actual_original = test_dataset.scaler_y.inverse_transform(actual_np).flatten()\n",
    "            \n",
    "            test_predictions.extend(pred_original)\n",
    "            test_actuals.extend(actual_original)\n",
    "    \n",
    "    test_predictions = np.array(test_predictions)\n",
    "    test_actuals = np.array(test_actuals)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = np.mean(np.abs(test_predictions - test_actuals))\n",
    "    rmse = np.sqrt(np.mean((test_predictions - test_actuals) ** 2))\n",
    "    r2 = 1 - (np.sum((test_actuals - test_predictions) ** 2) / \n",
    "              np.sum((test_actuals - np.mean(test_actuals)) ** 2))\n",
    "    \n",
    "    print(f\"üìä Final Test Performance:\")\n",
    "    print(f\"  MAE:  {mae:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  R¬≤:   {r2:.4f}\")\n",
    "    \n",
    "    # Create ensemble if requested\n",
    "    ensemble_filename = None\n",
    "    if config['CREATE_ENSEMBLE'] and len(results) >= config['ENSEMBLE_TOP_K']:\n",
    "        print(f\"\\n\" + \"=\"*40)\n",
    "        print(f\"CREATING ENSEMBLE MODEL (Top {config['ENSEMBLE_TOP_K']})\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        ensemble_model = create_ensemble_model(results, config['ENSEMBLE_TOP_K'])\n",
    "        ensemble_model = ensemble_model.to(device)\n",
    "        \n",
    "        # Evaluate ensemble\n",
    "        ensemble_model.eval()\n",
    "        ensemble_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                outputs = ensemble_model(batch_x)\n",
    "                pred_np = outputs.cpu().numpy().reshape(-1, 1)\n",
    "                pred_original = test_dataset.scaler_y.inverse_transform(pred_np).flatten()\n",
    "                ensemble_predictions.extend(pred_original)\n",
    "        \n",
    "        ensemble_predictions = np.array(ensemble_predictions)\n",
    "        \n",
    "        # Ensemble metrics\n",
    "        ensemble_mae = np.mean(np.abs(ensemble_predictions - test_actuals))\n",
    "        ensemble_rmse = np.sqrt(np.mean((ensemble_predictions - test_actuals) ** 2))\n",
    "        ensemble_r2 = 1 - (np.sum((test_actuals - ensemble_predictions) ** 2) / \n",
    "                          np.sum((test_actuals - np.mean(test_actuals)) ** 2))\n",
    "        \n",
    "        print(f\"üìä Ensemble Performance:\")\n",
    "        print(f\"  MAE:  {ensemble_mae:.4f}\")\n",
    "        print(f\"  RMSE: {ensemble_rmse:.4f}\")\n",
    "        print(f\"  R¬≤:   {ensemble_r2:.4f}\")\n",
    "        \n",
    "        # Save ensemble model\n",
    "        ensemble_filename = f'ensemble_model_{timestamp}.pth'\n",
    "        torch.save(ensemble_model.state_dict(), ensemble_filename)\n",
    "        print(f\"üíæ Ensemble model saved as '{ensemble_filename}'\")\n",
    "    \n",
    "    # Create visualization plots\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"CREATING VISUALIZATIONS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    try:\n",
    "        plot_filename = f'nas_results_visualization_{timestamp}.png'\n",
    "        \n",
    "        plt.figure(figsize=(18, 6))\n",
    "        \n",
    "        # Plot 1: Actual vs Predicted\n",
    "        plt.subplot(1, 3, 1)\n",
    "        sample_size = min(500, len(test_actuals))\n",
    "        plt.scatter(test_actuals[:sample_size], test_predictions[:sample_size], alpha=0.6, s=10)\n",
    "        min_val, max_val = min(test_actuals.min(), test_predictions.min()), max(test_actuals.max(), test_predictions.max())\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "        plt.xlabel(f'Actual {target_column}')\n",
    "        plt.ylabel(f'Predicted {target_column}')\n",
    "        plt.title(f'Actual vs Predicted\\n(R¬≤ = {r2:.3f})')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Time series comparison\n",
    "        plt.subplot(1, 3, 2)\n",
    "        comparison_length = min(200, len(test_actuals))\n",
    "        time_steps = range(comparison_length)\n",
    "        plt.plot(time_steps, test_actuals[:comparison_length], label='Actual', linewidth=2)\n",
    "        plt.plot(time_steps, test_predictions[:comparison_length], label='Predicted', linewidth=2, alpha=0.8)\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel(f'{target_column} Value')\n",
    "        plt.title(f'Time Series Comparison\\n(MAE = {mae:.3f})')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Error distribution\n",
    "        plt.subplot(1, 3, 3)\n",
    "        errors = test_predictions - test_actuals\n",
    "        plt.hist(errors, bins=50, alpha=0.7, edgecolor='black', color='skyblue')\n",
    "        plt.xlabel('Prediction Error')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'Error Distribution\\n(RMSE = {rmse:.3f})')\n",
    "        plt.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
    "        plt.axvline(x=np.mean(errors), color='orange', linestyle='-', linewidth=2, label=f'Mean Error = {np.mean(errors):.3f}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üìä Visualizations saved as '{plot_filename}'\")\n",
    "        \n",
    "        # Try to show the plot\n",
    "        try:\n",
    "            plt.show()\n",
    "        except:\n",
    "            print(\"üìä Plot saved but cannot display in this environment\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not create visualizations: {e}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"‚úÖ Neural Architecture Search completed successfully!\")\n",
    "    print(f\"üìä Dataset: {len(feature_columns)} features, {target_column} target\")\n",
    "    print(f\"üîç Searched {config['NUM_TRIALS']} architectures\")\n",
    "    print(f\"üèÜ Best architecture performance:\")\n",
    "    print(f\"   ‚Ä¢ Validation Loss: {best_result['val_loss']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Test MAE: {mae:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Test RMSE: {rmse:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Test R¬≤: {r2:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Model Parameters: {best_result['total_params']:,}\")\n",
    "    print(f\"   ‚Ä¢ Inference Time: {best_result['inference_time_ms']:.2f}ms\")\n",
    "    \n",
    "    print(f\"\\nüèóÔ∏è  Best Architecture:\")\n",
    "    print(f\"   ‚Ä¢ Layers: {best_config.layer_types}\")\n",
    "    print(f\"   ‚Ä¢ Hidden Dimensions: {best_config.hidden_dims}\")\n",
    "    print(f\"   ‚Ä¢ Attention Heads: {best_config.attention_heads}\")\n",
    "    print(f\"   ‚Ä¢ Skip Connections: {best_config.use_skip_connections}\")\n",
    "    print(f\"   ‚Ä¢ Activation: {best_config.activation}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Files created:\")\n",
    "    print(f\"   ‚Ä¢ Model: {model_filename}\")\n",
    "    print(f\"   ‚Ä¢ Results: {results_filename}\")\n",
    "    print(f\"   ‚Ä¢ Visualization: {plot_filename}\")\n",
    "    if config['CREATE_ENSEMBLE'] and len(results) >= config['ENSEMBLE_TOP_K']:\n",
    "        print(f\"   ‚Ä¢ Ensemble: {ensemble_filename}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ To use your trained model:\")\n",
    "    print(f\"   model = DynamicINSModel(best_config)\")\n",
    "    print(f\"   model.load_state_dict(torch.load('{model_filename}'))\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"NAS COMPLETED SUCCESSFULLY! üéâ\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# UTILITY FUNCTIONS FOR LOADING SAVED MODELS\n",
    "# ====================================================================\n",
    "\n",
    "def load_trained_model(model_path: str, config_path: str):\n",
    "    \"\"\"\n",
    "    Load a previously trained model from NAS results.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model (.pth file)\n",
    "        config_path: Path to the NAS results (.json file)\n",
    "    \n",
    "    Returns:\n",
    "        model: Loaded PyTorch model\n",
    "        config: Architecture configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the configuration\n",
    "    with open(config_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    best_config_dict = results[0]['config']  # Best performing architecture\n",
    "    \n",
    "    # Reconstruct ArchConfig\n",
    "    best_config = ArchConfig(\n",
    "        sequence_length=best_config_dict['sequence_length'],\n",
    "        input_features=best_config_dict['input_features'],\n",
    "        hidden_dims=best_config_dict['hidden_dims'],\n",
    "        layer_types=best_config_dict['layer_types'],\n",
    "        attention_heads=best_config_dict['attention_heads'],\n",
    "        dropout_rates=best_config_dict['dropout_rates'],\n",
    "        use_skip_connections=best_config_dict['use_skip_connections'],\n",
    "        activation=best_config_dict['activation'],\n",
    "        output_dim=best_config_dict['output_dim']\n",
    "    )\n",
    "    \n",
    "    # Load the model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = DynamicINSModel(best_config).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    return model, best_config\n",
    "\n",
    "\n",
    "def predict_with_trained_model(model, data, feature_columns, sequence_length, scaler_X, scaler_y):\n",
    "    \"\"\"\n",
    "    Make predictions using a trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        data: Input data (DataFrame)\n",
    "        feature_columns: List of feature column names\n",
    "        sequence_length: Sequence length used during training\n",
    "        scaler_X: Fitted feature scaler from training\n",
    "        scaler_y: Fitted target scaler from training\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Predicted values (original scale)\n",
    "    \"\"\"\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Prepare input data\n",
    "    X = data[feature_columns].values\n",
    "    X_scaled = scaler_X.transform(X)\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = []\n",
    "    for i in range(len(X_scaled) - sequence_length + 1):\n",
    "        seq = X_scaled[i:i + sequence_length]\n",
    "        sequences.append(seq)\n",
    "    \n",
    "    if not sequences:\n",
    "        raise ValueError(f\"Not enough data for sequence length {sequence_length}\")\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(sequences).to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_size = 32  # Process in batches to avoid memory issues\n",
    "        for i in range(0, len(X_tensor), batch_size):\n",
    "            batch = X_tensor[i:i + batch_size]\n",
    "            outputs = model(batch)\n",
    "            batch_predictions = outputs.cpu().numpy()\n",
    "            predictions.extend(batch_predictions.flatten())\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    predictions = np.array(predictions).reshape(-1, 1)\n",
    "    predictions_original = scaler_y.inverse_transform(predictions).flatten()\n",
    "    \n",
    "    return predictions_original\n",
    "\n",
    "\n",
    "# Advanced ensemble method\n",
    "class EnsembleINSModel(nn.Module):\n",
    "    \"\"\"Ensemble of top-k architectures from NAS\"\"\"\n",
    "    \n",
    "    def __init__(self, configs: List[ArchConfig], weights: List[float] = None):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([DynamicINSModel(config) for config in configs])\n",
    "        self.weights = weights if weights else [1.0] * len(configs)\n",
    "        self.weights = torch.FloatTensor(self.weights)\n",
    "        self.weights = self.weights / self.weights.sum()  # Normalize weights\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for model in self.models:\n",
    "            outputs.append(model(x))\n",
    "        \n",
    "        # Weighted average\n",
    "        stacked_outputs = torch.stack(outputs, dim=0)  # (num_models, batch_size, output_dim)\n",
    "        weights = self.weights.to(x.device).view(-1, 1, 1)\n",
    "        weighted_output = (stacked_outputs * weights).sum(dim=0)\n",
    "        \n",
    "        return weighted_output\n",
    "\n",
    "\n",
    "def create_ensemble_model(nas_results: List[Dict], top_k: int = 3):\n",
    "    \"\"\"Create ensemble from top-k NAS results\"\"\"\n",
    "    \n",
    "    # Get top-k configurations\n",
    "    top_configs = [result['config'] for result in nas_results[:top_k]]\n",
    "    \n",
    "    # Calculate ensemble weights based on validation performance\n",
    "    val_losses = [result['val_loss'] for result in nas_results[:top_k]]\n",
    "    \n",
    "    # Convert losses to weights (lower loss = higher weight)\n",
    "    max_loss = max(val_losses)\n",
    "    weights = [max_loss - loss for loss in val_losses]\n",
    "    \n",
    "    # Handle case where all losses are the same\n",
    "    if sum(weights) == 0:\n",
    "        weights = [1.0] * len(val_losses)\n",
    "    \n",
    "    return EnsembleINSModel(top_configs, weights)\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility class\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# SIMPLE USAGE EXAMPLE\n",
    "# ====================================================================\n",
    "\n",
    "def simple_usage_example():\n",
    "    \"\"\"\n",
    "    Simple example of how to use this code with your data\n",
    "    \"\"\"\n",
    "    print(\"\"\"\n",
    "SIMPLE USAGE INSTRUCTIONS:\n",
    "\n",
    "1. Modify the 'configure_your_data()' function:\n",
    "   - Set your CSV file path\n",
    "   - Set your target column name\n",
    "   - Set your feature column names\n",
    "\n",
    "2. Or modify the 'get_training_config()' function:\n",
    "   - Set YOUR_TARGET_COLUMN to your target column name\n",
    "   - Set YOUR_FEATURE_COLUMNS to your feature column names\n",
    "\n",
    "3. Run the script:\n",
    "   python nas_ins_model.py\n",
    "\n",
    "Example configuration for get_training_config():\n",
    "\n",
    "def get_training_config():\n",
    "    config = {\n",
    "        # ... other settings ...\n",
    "        'YOUR_TARGET_COLUMN': 'velocity',  # Your actual target column\n",
    "        'YOUR_FEATURE_COLUMNS': ['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y']  # Your features\n",
    "    }\n",
    "    return config\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if user needs help\n",
    "    import sys\n",
    "    if len(sys.argv) > 1 and sys.argv[1] in ['-h', '--help', 'help']:\n",
    "        simple_usage_example()\n",
    "    else:\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
